<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Mask R-CNN论文解读</title>
      <link href="/2019/05/05/mask-r-cnn-lun-wen-jie-du/"/>
      <url>/2019/05/05/mask-r-cnn-lun-wen-jie-du/</url>
      
        <content type="html"><![CDATA[<h1 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h1><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p>何凯明 Geeorgia Gkioxari Piotr Dollar Ross Girshick</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>在本文中，作者提出了一种用于目标实例分割的方法。该方法在检测目标的同时针对每一个目标实例产生一个高质量的分割蒙板。Mask R-CNN通过在Faster R-CNN现有的用于目标检测的分支的基础上添加用于目标mask预测的分支实现。</p><h2 id="目标检测、目标实例分割、语义分割"><a href="#目标检测、目标实例分割、语义分割" class="headerlink" title="目标检测、目标实例分割、语义分割"></a>目标检测、目标实例分割、语义分割</h2><p>如下图所示：</p><center><br><img src="https://img-blog.csdn.net/20171121232307984?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveGlhbWVudGluZ3Rhbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br><br></center><ul><li>图片分类<br>仅需要识别出一张图片中存在哪几类目标即可。</li><li>目标检测<br>需要给出图片中目标的类别和具体位置。</li><li>语义分割<br>对图片中的目标进行像素级分割，但只需要区分不同类别目标即可，统一类别的目标不需要区分。</li><li>实例分割<br>对图片中的目标进行像素级分割，但需要区分不同的实例，同一类别的不同个体同样需要进行区分。</li></ul><h2 id="Mask-R-CNN-解决实例分割问题"><a href="#Mask-R-CNN-解决实例分割问题" class="headerlink" title="Mask R-CNN:解决实例分割问题"></a>Mask R-CNN:解决实例分割问题</h2><p><strong>R-CNN的网络结构：</strong></p><center><br><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/5d9736d2-0b1a-43fa-912a-60be077b0387.jpg"><br></center><p><strong>Fast R-CNN的网络结构：</strong></p><center><br><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/b65ae552-1ac7-478c-91d8-82ae6e23d285.jpg"><br><br></center><p><strong>Faster R-CNN的网络结构</strong></p><center><br><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/b19b660d-8e82-48cf-817d-1429475f15be.jpg"><br><br></center><p><strong>Mask R-CNN的总体框架如图所示：</strong></p><center><br><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/27847427-802c-4928-8626-e535429eae7a.jpg"><br><br></center><br><center><br><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/d1f5ad89-4152-4cf0-b22b-6b307397dee6.jpg"><br><br></center><p>作者在Faster R-CNN原有的用于预测目标bounding boxes的子网络的基础上，添加用于预测mask的分支，使用FCN(全卷积网络)对每一个RoI分别进行预测。Faster R-CNN并不是为网络输入与输出之间的像素级的匹配而设计的，这一问题主要是由RoIPool层的空间量化操作所导致的。为了解决这一问题，作者提出了一种简单的、无需量化的层，即RoIAlign，该层的引入极大地保证了空间位置地准确性。RoIAlign层对最终的检测结果有着极大的影响。<br>除此之外，作者发现很有必要对mask预测和类别预测进行解耦和。针对每一类分别预测一层mask，类别之间不存在竞争关系，将类别预测任务交给RoI的分类分支。</p><h3 id="RoIAlign"><a href="#RoIAlign" class="headerlink" title="RoIAlign"></a>RoIAlign</h3><p>给定特征图如下所示：</p><center><br><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/ee623a5f-c1f5-4a2a-98f7-4c94fa379a3a.jpg"><br><br></center><p>传统的RoIPool层针对每一个RoI分别产生一层小的特征图。RoIPool的步骤如下：</p><ol><li>首先对RoI的浮点坐标、大小参数进行量化，将其对应到特征图中。<br><center><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/bf8dcd4c-4274-44c7-ab9d-0ed1ebea64fa.jpg"></center></li></ol><p></p><ol start="2"><li>接着经过量化的RoI将被划分为格子，针对每一个格子内部进行池化操作，进而得到固定大小的特征图。<br><center><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/82c99a99-edf8-4c09-9e5c-b7f9ea190722.jpg"></center></li></ol><p></p><p>在上述操作中，共存在两步取整操作，一个是将RoI对应至特征图时，一个是对量化后的RoI进行划分。这两步取整量化操作会导致原始RoI与抽取出的特征图在空间位置上不匹配。这一问题不会对目标的分类造成大的影响，但会对mask预测造成极大的负面影响。</p><p>为了解决这一问题，作者提出了RoIAlign层，RoIAlign去除了量化取整操作，使得抽取的特征图与输入图片有着精确的位置对应。对于RoI中的每一个格子，使用双线性插值法计算其对应的值，双线性插值法需要的原始值来自于格子四角位置上的值。如下图所示：</p><center><br><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/86330610-af9b-4205-9736-3240e2bcadb2.jpg"><br><br></center><br>整体步骤如下：<br>1. 使用浮点运算，将RoI对应至特征图的相应位置<br><center><br><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/2e9be7aa-3b8d-4bc1-ab91-c7a24239e866.jpg"><br><br></center><ol start="2"><li>将每一个格子划分为四个小格子<br><center><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/6424cdaa-67a6-4774-8d71-662bbba20f6c.jpg"></center></li></ol><p></p><ol start="3"><li>使用双线性插值法计算每一个格子的值，取四角的值为原始值<br><center></center></li></ol><p></p><ol start="4"><li>对每一个格子进行池化操作，得到最终结果<br><center></center></li></ol><p></p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数由类别损失、边框回归损失、mask损失三部分构成，与其他方法不同，计算mask损失时，对预测的各个类别的mask分别使用sigmoid函数进行激活（而不是使用softmax函数对所有类别的mask进行激活），接着使用二维交叉熵计算损失。</p><h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><p>用于预测mask的子网络的结构如图所示：</p><center><br><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/e0da9a7e-36b8-468e-8860-db023a099a91.jpg"><br><br></center><p>左侧使用resnet-c4作为前面的卷积网络，将rpn生成的roi映射到C4的输出，并进行roi pooling，最后进行分叉预测三个目标。右侧即使用Faster R-CNN加FPN的结构。</p><h3 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h3><p>与FCIS+++的对比，如下图所示：</p><p><center><br><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/170fbc97-7e4f-44bd-bf89-dbeae157b64a.jpg"></center></p><p><br>在FCIS++的预测中会在目标重合位置出现一条直线，而Mask R-CNN的预测结果则没有。</p><p><strong>消融实验</strong><br>结果如图所示：</p><p><center><br><img src="/2019/05/05/mask-r-cnn-lun-wen-jie-du/3095a77a-a0f9-4b7f-9482-42fde3b002fd.jpg"></center></p><p><br>作者分别给出了不同backbone、多任务和独立任务、使用RoIAligh和不使用、使用FPN进行结果预测和使用全连接层的对比结果。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> 实例分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSD论文阅读及核心代码解析</title>
      <link href="/2019/04/30/ssd-lun-wen-yue-du-ji-he-xin-dai-ma-jie-xi/"/>
      <url>/2019/04/30/ssd-lun-wen-yue-du-ji-he-xin-dai-ma-jie-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="SSD-Single-Shot-MultiBox-Detector"><a href="#SSD-Single-Shot-MultiBox-Detector" class="headerlink" title="SSD:Single Shot MultiBox Detector"></a>SSD:Single Shot MultiBox Detector</h2><hr><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p><strong>Wei Liu</strong></p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>在本文中,作者提出了一种使用单个神经网络进行目标检测的框架,该框架的特点如下:</p><ol><li><p>将网络的bounding boxes的输出空间划分为default boxes的集合,这些boxes具有不同的尺度和比例.对于被选中用于目标预测的feature maps,网络会针对该特征图中的每一个位置预测多个default boxes,这些default boxes又称为anchors.<br>对于每一个default box,网络给出其存在目标的分数(每一个类别分别预测一个),同时给出在default box的基础上进行形状调整的参数.</p></li><li><p>为了应对目标的尺度变化,SSD在来自于网络的多个具有不同分辨率的feature maps上进行预测.其中,靠近网络前侧,具有较高分辨率的feature map适用于进行小目标的检测,而后侧的feature maps适用于大目标的检测.</p></li></ol><p>作者将本文的贡献总结如下:</p><ol><li>SSD的核心是:在事先设定的固定大小,比例的default boxes的基础上,使用较小的卷积核预测目标隶属于某一类的分数以及box的偏差.</li><li>为了获得较高的检测准确率,在具有不同大小的feature maps上产生具有不同尺度的预测,且这些预测又可以具有不同的长宽比.</li><li>网络可以进行end-to-end训练,同时取得不错的准确率.</li></ol><h2 id="SSD网络结构"><a href="#SSD网络结构" class="headerlink" title="SSD网络结构"></a>SSD网络结构</h2><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>SSD方法基于前馈神经网络,该网络产生固定数目的bounding boxes集合,同时给出这些boxes中存在某一类别的目标的分数,在这些bounding boxes的基础上,使用NMS算法求出最后的结果.网络的结构如下:</p><center><br><img src="/2019/04/30/ssd-lun-wen-yue-du-ji-he-xin-dai-ma-jie-xi/SSD网络结构.png" title="SSD网络结构"><br></center><p>由上图可以发现,总共选取了6层具有不同大小的feature maps用于目标检测,与R-CNN等方法使用全连接层给出预测结果不同,SSD使用较小的卷积计算得到预测结果.对于每一层feature map,使用一个卷积核计算得到各个default boxes属于某一类的分数,使用另一个卷积核得到在各个default boxes位置的基础上的偏差.</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h4 id="匹配策略"><a href="#匹配策略" class="headerlink" title="匹配策略"></a>匹配策略</h4><p>在进行训练的时候,需要决定哪一个default boxes与ground truth相关联,只有被选中的default boxes才会参与训练.所采取的策略为:<strong>首先将每一个ground truth和与其覆盖率最大的default boxes进行匹配,接着将与ground truth的jaccard重合率高于设定阈值的default boxes视为匹配</strong>,这样,在第一步保证每一个ground truth都会有匹配的default boxes的基础上,第二步使得多个default boxes可以匹配同一个ground truth.这一设定简化了学习问题,使得网络可以对多个重合的default boxes给出较高的预测分数,而不是仅仅选择具有最高重合率的一个.<br>匹配代码如下:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># ./layers/box_utils.py</span><span class="token keyword">def</span> <span class="token function">point_form</span><span class="token punctuation">(</span>boxes<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 将(cx, cy, w, h) 形式的box坐标转换成 (xmin, ymin, xmax, ymax) 形式</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span> <span class="token punctuation">(</span>boxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">-</span> boxes<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># xmin, ymin</span>                    <span class="token punctuation">(</span>boxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> boxes<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># xmax, ymax</span><span class="token keyword">def</span> <span class="token function">intersect</span><span class="token punctuation">(</span>box_a<span class="token punctuation">,</span> box_b<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># box_a: (truths), (tensor:[num_obj, 4])</span>    <span class="token comment" spellcheck="true"># box_b: (priors), (tensor:[num_priors, 4], 即[8732, 4])</span>    <span class="token comment" spellcheck="true"># return: (tensor:[num_obj, num_priors]) box_a 与 box_b 两个集合中任意两个 box 的交集, 其中res[i][j]代表box_a中第i个box与box_b中第j个box的交集.(非对称矩阵)</span>    <span class="token comment" spellcheck="true"># 思路: 先将两个box的维度扩展至相同维度: [num_obj, num_priors, 4], 然后计算面积的交集</span>    <span class="token comment" spellcheck="true"># 两个box的交集可以看成是一个新的box, 该box的左上角坐标是box_a和box_b左上角坐标的较大值, 右下角坐标是box_a和box_b的右下角坐标的较小值</span>    A <span class="token operator">=</span> box_a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    B <span class="token operator">=</span> box_b<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># box_a 左上角/右下角坐标 expand以后, 维度会变成(A,B,2), 其中, 具体可看 expand 的相关原理. box_b也是同理, 这样做是为了得到a中某个box与b中某个box的左上角(min_xy)的较大者(max)</span>    <span class="token comment" spellcheck="true"># unsqueeze 为增加维度的数量, expand 为扩展维度的大小</span>    min_xy <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>box_a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>A<span class="token punctuation">,</span>B<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                        box_b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>A<span class="token punctuation">,</span>B<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 在box_a的 A 和 2 之间增加一个维度, 并将维度扩展到 B. box_b 同理</span>    <span class="token comment" spellcheck="true"># 求右下角(max_xy)的较小者(min)</span>    max_xy <span class="token operator">=</span> torch<span class="token punctuation">.</span>min<span class="token punctuation">(</span>box_a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>A<span class="token punctuation">,</span>B<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                        box_b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>A<span class="token punctuation">,</span>B<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    inter <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token punctuation">(</span>max_xy<span class="token punctuation">,</span> min_xy<span class="token punctuation">)</span><span class="token punctuation">,</span> min<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 右下角减去左上角, 如果为负值, 说明没有交集, 置为0</span>    <span class="token keyword">return</span> inter<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> inter<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 高×宽, 返回交集的面积, shape 刚好为 [A, B]</span><span class="token keyword">def</span> <span class="token function">jaccard</span><span class="token punctuation">(</span>box_a<span class="token punctuation">,</span> box_b<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)</span>    <span class="token comment" spellcheck="true"># box_a: (truths), (tensor:[num_obj, 4])</span>    <span class="token comment" spellcheck="true"># box_b: (priors), (tensor:[num_priors, 4], 即[8732, 4])</span>    <span class="token comment" spellcheck="true"># return: (tensor:[num_obj, num_priors]), 代表了 box_a 和 box_b 两个集合中任意两个 box之间的交并比</span>    inter <span class="token operator">=</span> intersect<span class="token punctuation">(</span>box_a<span class="token punctuation">,</span> box_b<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 求任意两个box的交集面积, shape为[A, B], 即[num_obj, num_priors]</span>    area_a <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>box_a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">-</span>box_a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>box_a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">-</span>box_a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>inter<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [A,B]</span>    area_b <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>box_b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">-</span>box_b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>box_b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">-</span>box_b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>inter<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [A,B], 这里会将A中的元素复制B次</span>    union <span class="token operator">=</span> area_a <span class="token operator">+</span> area_b <span class="token operator">-</span> inter    <span class="token keyword">return</span> inter <span class="token operator">/</span> union <span class="token comment" spellcheck="true"># [A, B], 返回任意两个box之间的交并比, res[i][j] 代表box_a中的第i个box与box_b中的第j个box之间的交并比.</span><span class="token keyword">def</span> <span class="token function">encode</span><span class="token punctuation">(</span>matched<span class="token punctuation">,</span> priors<span class="token punctuation">,</span> variances<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 对边框坐标进行编码, 需要宽度方差和高度方差两个参数, 具体公式可以参见原文公式(2)</span>    <span class="token comment" spellcheck="true"># matched: [num_priors,4] 存储的是与priorbox匹配的gtbox的坐标. 形式为(xmin, ymin, xmax, ymax)</span>    <span class="token comment" spellcheck="true"># priors: [num_priors, 4] 存储的是priorbox的坐标. 形式为(cx, cy, w, h)</span>    <span class="token comment" spellcheck="true"># return : encoded boxes: [num_priors, 4]</span>    g_cxy <span class="token operator">=</span> <span class="token punctuation">(</span>matched<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> matched<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span> <span class="token operator">-</span> priors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 用互相匹配的gtbox的中心坐标减去priorbox的中心坐标, 获得中心坐标的偏移量</span>    g_cxy <span class="token operator">/=</span> <span class="token punctuation">(</span>variances<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">*</span>priors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 令中心坐标分别除以 d_i^w 和 d_i^h, 正如原文公式所示</span>    <span class="token comment" spellcheck="true">#variances[0]为0.1, 令其分别乘以w和h, 得到d_i^w 和 d_i^h</span>    g_wh <span class="token operator">=</span> <span class="token punctuation">(</span>matched<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">-</span> matched<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> priors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 令互相匹配的gtbox的宽高除以priorbox的宽高.</span>    g_wh <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>g_wh<span class="token punctuation">)</span> <span class="token operator">/</span> variances<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 这里这个variances[1]=0.2 不太懂是为什么.</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>g_cxy<span class="token punctuation">,</span> g_wh<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 将编码后的中心坐标和宽高``连接起来, 返回 [num_priors, 4]</span><span class="token keyword">def</span> <span class="token function">match</span><span class="token punctuation">(</span>threshold<span class="token punctuation">,</span> truths<span class="token punctuation">,</span> priors<span class="token punctuation">,</span> variances<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> loc_t<span class="token punctuation">,</span> conf_t<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># threshold: (float) 确定是否匹配的交并比阈值</span>    <span class="token comment" spellcheck="true"># truths: (tensor: [num_obj, 4]) 存储真实 box 的边框坐标</span>    <span class="token comment" spellcheck="true"># priors: (tensor: [num_priors, 4], 即[8732, 4]), 存储推荐框的坐标, 注意, 此时的框是 default box, 而不是 SSD 网络预测出来的框的坐标, 预测的结果存储在 loc_data中, 其 shape 为[num_obj, 8732, 4].</span>    <span class="token comment" spellcheck="true"># variances: cfg['variance'], [0.1, 0.2], 用于将坐标转换成方便训练的形式(参考RCNN系列对边框坐标的处理)</span>    <span class="token comment" spellcheck="true"># labels: (tensor: [num_obj]), 代表了每个真实 box 对应的类别的编号</span>    <span class="token comment" spellcheck="true"># loc_t: (tensor: [batches, 8732, 4]),</span>    <span class="token comment" spellcheck="true"># conf_t: (tensor: [batches, 8732]),</span>    <span class="token comment" spellcheck="true"># idx: batches 中图片的序号, 标识当前正在处理的 image 在 batches 中的序号</span>    overlaps <span class="token operator">=</span> jaccard<span class="token punctuation">(</span>truths<span class="token punctuation">,</span> point_form<span class="token punctuation">(</span>priors<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [A, B], 返回任意两个box之间的交并比, overlaps[i][j] 代表box_a中的第i个box与box_b中的第j个box之间的交并比.</span>    <span class="token comment" spellcheck="true"># 二部图匹配(Bipartite Matching)</span>    <span class="token comment" spellcheck="true"># [num_objs,1], 得到对于每个 gt box 来说的匹配度最高的 prior box, 前者存储交并比, 后者存储prior box在num_priors中的位置</span>    best_prior_overlap<span class="token punctuation">,</span> best_prior_idx <span class="token operator">=</span> overlaps<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># keepdim=True, 因此shape为[num_objs,1]</span>    <span class="token comment" spellcheck="true"># [1, num_priors], 即[1,8732], 同理, 得到对于每个 prior box 来说的匹配度最高的 gt box</span>    best_truth_overlap<span class="token punctuation">,</span> best_truth_idx <span class="token operator">=</span> overlaps<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    best_prior_idx<span class="token punctuation">.</span>squeeze_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 上面特意保留了维度(keepdim=True), 这里又都把维度 squeeze/reduce 了, 实际上只需用默认的 keepdim=False 就可以自动 squeeze/reduce 维度.</span>    best_prior_overlap<span class="token punctuation">.</span>squeeze_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    best_truth_idx<span class="token punctuation">.</span>squeeze_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    best_truth_overlap<span class="token punctuation">.</span>squeeze_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    best_truth_overlap<span class="token punctuation">.</span>index_fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> best_prior_idx<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 维度压缩后变为[num_priors], best_prior_idx 维度为[num_objs],</span>    <span class="token comment" spellcheck="true"># 该语句会将与gt box匹配度最好的prior box 的交并比置为 2, 确保其最大, 以免防止某些 gtbox 没有匹配的 priorbox.</span>    <span class="token comment" spellcheck="true"># 假想一种极端情况, 所有的priorbox与某个gtbox(标记为G)的交并比为1, 而其他gtbox分别有一个交并比</span>    <span class="token comment" spellcheck="true"># 最高的priorbox, 但是肯定小于1(因为其他的gtbox与G的交并比肯定小于1), 这样一来, 就会使得所有</span>    <span class="token comment" spellcheck="true"># 的priorbox都与G匹配, 为了防止这种情况, 我们将那些对gtbox来说, 具有最高交并比的priorbox,</span>    <span class="token comment" spellcheck="true"># 强制进行互相匹配, 即令best_truth_idx[best_prior_idx[j]] = j, 详细见下面的for循环</span>    <span class="token comment" spellcheck="true"># 注意!!: 因为 gt box 的数量要远远少于 prior box 的数量, 因此, 同一个 gt box 会与多个 prior box 匹配.</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>best_prior_idx<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># range:0~num_obj-1</span>        best_truth_idx<span class="token punctuation">[</span>best_prior_idx<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> j        <span class="token comment" spellcheck="true"># best_prior_idx[j] 代表与box_a的第j个box交并比最高的 prior box 的下标, 将与该 gtbox</span>        <span class="token comment" spellcheck="true"># 匹配度最好的 prior box 的下标改为j, 由此,完成了该 gtbox 与第j个 prior box 的匹配.</span>        <span class="token comment" spellcheck="true"># 这里的循环只会进行num_obj次, 剩余的匹配为 best_truth_idx 中原本的值.</span>        <span class="token comment" spellcheck="true"># 这里处理的情况是, priorbox中第i个box与gtbox中第k个box的交并比最高,</span>        <span class="token comment" spellcheck="true"># 即 best_truth_idx[i]= k</span>        <span class="token comment" spellcheck="true"># 但是对于best_prior_idx[k]来说, 它却与priorbox的第l个box有着最高的交并比,</span>        <span class="token comment" spellcheck="true"># 即best_prior_idx[k]=l</span>        <span class="token comment" spellcheck="true"># 而对于gtbox的另一个边框gtbox[j]来说, 它与priorbox[i]的交并比最大,</span>        <span class="token comment" spellcheck="true"># 即但是对于best_prior_idx[j] = i.</span>        <span class="token comment" spellcheck="true"># 那么, 此时, 我们就应该将best_truth_idx[i]= k 修改成 best_truth_idx[i]= j.</span>        <span class="token comment" spellcheck="true"># 即令 priorbox[i] 与 gtbox[j]对应.</span>        <span class="token comment" spellcheck="true"># 这样做的原因: 防止某个gtbox没有匹配的 prior box.</span>    mathes <span class="token operator">=</span> truths<span class="token punctuation">[</span>best_truth_idx<span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># truths 的shape 为[num_objs, 4], 而best_truth_idx是一个指示下标的列表, 列表长度为 8732,</span>    <span class="token comment" spellcheck="true"># 列表中的下标范围为0~num_objs-1, 代表的是与每个priorbox匹配的gtbox的下标</span>    <span class="token comment" spellcheck="true"># 上面的表达式会返回一个shape为 [num_priors, 4], 即 [8732, 4] 的tensor, 代表的就是与每个priorbox匹配的gtbox的坐标值.</span>    conf <span class="token operator">=</span> labels<span class="token punctuation">[</span>best_truth_idx<span class="token punctuation">]</span><span class="token operator">+</span><span class="token number">1</span> <span class="token comment" spellcheck="true"># 与上面的语句道理差不多, 这里得到的是每个prior box匹配的类别编号, shape 为[8732]</span>    conf<span class="token punctuation">[</span>best_truth_overlap <span class="token operator">&lt;</span> threshold<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment" spellcheck="true"># 将与gtbox的交并比小于阈值的置为0 , 即认为是非物体框</span>    loc <span class="token operator">=</span> encode<span class="token punctuation">(</span>matches<span class="token punctuation">,</span> priors<span class="token punctuation">,</span> variances<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 返回编码后的中心坐标和宽高.</span>    loc_t<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> loc <span class="token comment" spellcheck="true"># 设置第idx张图片的gt编码坐标信息</span>    conf_t<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> conf <span class="token comment" spellcheck="true"># 设置第idx张图片的编号信息.(大于0即为物体编号, 认为有物体, 小于0认为是背景)</span></code></pre><p><strong>代码流程为</strong>:</p><ol><li>计算出每一个ground truth与每一个default boxes的jaccard overlap;</li><li>挑出与每一个ground truth最匹配(重复度最高)的default boxes;</li><li>挑出与每一个default boxes最匹配的ground truth;</li><li>注意,最终的匹配结果要保证在每一个ground truth都有与之匹配的default boxes的基础上,可以存在多个default boxes匹配同一个ground truth,这就是<pre><code>for j in range(best_prior_idx.size(0)):   best_truth_idx[best_prior_idx[j]] = j</code></pre> 这一for循环完成的功能.</li><li>将重复率低于阈值的标记为背景目标.</li></ol><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>模型的整体损失函数如下:</p><center><br>$L ( x , c , l , g ) = \frac { 1 } { N } \left( L _ { c o n f } ( x , c ) + \alpha L _ { l o c } ( x , l , g ) \right)$<br></center><p>其中N表示匹配的default boxes的数目,其中定位误差的计算方式如下:</p><center><br>$L _ { l o c } ( x , l , g ) = \sum _ { i \in P o s } ^ { N } \sum _ { m \in { c x , c y , w , h } } x _ { i j } ^ { k } \operatorname { smooth } _ { \mathrm { LI } } \left( l _ { i } ^ { m } - \hat { g } _ { j } ^ { m } \right)$<br><br>$\hat { g } _ { j } ^ { c x } = \left( g _ { j } ^ { c x } - d _ { i } ^ { c x } \right) / d _ { i } ^ { w } \quad \hat { g } _ { j } ^ { c y } = \left( g _ { j } ^ { c y } - d _ { i } ^ { c y } \right) / d _ { i } ^ { h }$<br><br>$\hat { g } _ { j } ^ { w } = \log \left( \frac { g _ { j } ^ { w } } { d _ { i } ^ { w } } \right) \quad \hat { g } _ { j } ^ { h } = \log \left( \frac { g _ { j } ^ { h } } { d _ { i } ^ { h } } \right)$<br></center><p>与Faster R-CNN类似,预测相对于default boxes的中心坐标的偏差,以及其宽和高.其中$x_ij^p$表示,将第i个default boxes与类别p的第j个ground truth进行匹配,这里,要将ground truth转换为相对于default boxes的偏移量.<br>置信度损失如下:</p><center><br>$L _ { c o n f } ( x , c ) = - \sum _ { i \in P o s } ^ { N } x _ { i j } ^ { p } \log \left( \hat { c } _ { i } ^ { p } \right) - \sum _ { i \in N e g } \log \left( \hat { c } _ { i } ^ { 0 } \right) \quad  where  \quad \hat { c } _ { i } ^ { p } = \frac { \exp \left( c _ { i } ^ { p } \right) } { \sum _ { p } \exp \left( c _ { i } ^ { p } \right) }$<br></center><p>其中$\alpha$设置为1.<br>代码如下:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># layers/modules/multibox_loss.py</span><span class="token keyword">class</span> <span class="token class-name">MultiBoxLoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 计算目标:</span>    <span class="token comment" spellcheck="true"># 输出那些与真实框的iou大于一定阈值的框的下标.</span>    <span class="token comment" spellcheck="true"># 根据与真实框的偏移量输出localization目标</span>    <span class="token comment" spellcheck="true"># 用难样例挖掘算法去除大量负样本(默认正负样本比例为1:3)</span>    <span class="token comment" spellcheck="true"># 目标损失:</span>    <span class="token comment" spellcheck="true"># L(x,c,l,g) = (Lconf(x,c) + αLloc(x,l,g)) / N</span>    <span class="token comment" spellcheck="true"># 参数:</span>    <span class="token comment" spellcheck="true"># c: 类别置信度(class confidences)</span>    <span class="token comment" spellcheck="true"># l: 预测的框(predicted boxes)</span>    <span class="token comment" spellcheck="true"># g: 真实框(ground truth boxes)</span>    <span class="token comment" spellcheck="true"># N: 匹配到的框的数量(number of matched default boxes)</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> overlap_thresh<span class="token punctuation">,</span> prior_for_matching<span class="token punctuation">,</span> bkg_label<span class="token punctuation">,</span> neg_mining<span class="token punctuation">,</span> neg_pos<span class="token punctuation">,</span> neg_overlap<span class="token punctuation">,</span> encode_target<span class="token punctuation">,</span> use_gpu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>MultiBoxLoss<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>use_gpu <span class="token operator">=</span> use_gpu        self<span class="token punctuation">.</span>num_classes<span class="token operator">=</span> num_classes <span class="token comment" spellcheck="true"># 列表数</span>        self<span class="token punctuation">.</span>threshold <span class="token operator">=</span> overlap_thresh <span class="token comment" spellcheck="true"># 交并比阈值, 0.5</span>        self<span class="token punctuation">.</span>background_label <span class="token operator">=</span> bkg_label <span class="token comment" spellcheck="true"># 背景标签, 0</span>        self<span class="token punctuation">.</span>use_prior_for_matching <span class="token operator">=</span> prior_for_matching <span class="token comment" spellcheck="true"># True 没卵用</span>        self<span class="token punctuation">.</span>do_neg_mining <span class="token operator">=</span> neg_mining <span class="token comment" spellcheck="true"># True, 没卵用</span>        self<span class="token punctuation">.</span>negpos_ratio <span class="token operator">=</span> neg_pos <span class="token comment" spellcheck="true"># 负样本和正样本的比例, 3:1</span>        self<span class="token punctuation">.</span>neg_overlap <span class="token operator">=</span> neg_overlap <span class="token comment" spellcheck="true"># 0.5 判定负样本的阈值.</span>        self<span class="token punctuation">.</span>encode_target <span class="token operator">=</span> encode_target <span class="token comment" spellcheck="true"># False 没卵用</span>        self<span class="token punctuation">.</span>variance <span class="token operator">=</span> cfg<span class="token punctuation">[</span><span class="token string">"variance"</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> predictions<span class="token punctuation">,</span> targets<span class="token punctuation">)</span><span class="token punctuation">:</span>        loc_data<span class="token punctuation">,</span> conf_data<span class="token punctuation">,</span> priors <span class="token operator">=</span> predictions        <span class="token comment" spellcheck="true"># loc_data: [batch_size, 8732, 4]</span>        <span class="token comment" spellcheck="true"># conf_data: [batch_size, 8732, 21]</span>        <span class="token comment" spellcheck="true"># priors: [8732, 4] default box 对于任意的图片, 都是相同的, 因此无需带有 batch 维度</span>        num <span class="token operator">=</span> loc_data<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># num = batch_size</span>        priors <span class="token operator">=</span> priors<span class="token punctuation">[</span><span class="token punctuation">:</span>loc_data<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># loc_data.size(1) = 8732, 因此 priors 维持不变</span>        num_priors <span class="token operator">=</span> <span class="token punctuation">(</span>priors<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># num_priors = 8732</span>        num_classes <span class="token operator">=</span> self<span class="token punctuation">.</span>num_classes <span class="token comment" spellcheck="true"># num_classes = 21 (默认为voc数据集)</span>        <span class="token comment" spellcheck="true"># 将priors(default boxes)和ground truth boxes匹配</span>        loc_t <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>num<span class="token punctuation">,</span> num_priors<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># shape:[batch_size, 8732, 4]</span>        conf_t <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>num<span class="token punctuation">,</span> num_priors<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># shape:[batch_size, 8732]</span>        <span class="token keyword">for</span> idx <span class="token keyword">in</span> range<span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># targets是列表, 列表的长度为batch_size, 列表中每个元素为一个 tensor,</span>            <span class="token comment" spellcheck="true"># 其 shape 为 [num_objs, 5], 其中 num_objs 为当前图片中物体的数量, 第二维前4个元素为边框坐标, 最后一个元素为类别编号(1~20)</span>            truths <span class="token operator">=</span> targets<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data <span class="token comment" spellcheck="true"># [num_objs, 4]</span>            labels <span class="token operator">=</span> targets<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data <span class="token comment" spellcheck="true"># [num_objs] 使用的是 -1, 而不是 -1:, 因此, 返回的维度变少了</span>            defaults <span class="token operator">=</span> priors<span class="token punctuation">.</span>data <span class="token comment" spellcheck="true"># [8732, 4]</span>            <span class="token comment" spellcheck="true"># from ..box_utils import match</span>            <span class="token comment" spellcheck="true"># 关键函数, 实现候选框与真实框之间的匹配, 注意是候选框而不是预测结果框! 这个函数实现较为复杂, 会在后面着重讲解</span>            match<span class="token punctuation">(</span>self<span class="token punctuation">.</span>threshold<span class="token punctuation">,</span> truths<span class="token punctuation">,</span> defaults<span class="token punctuation">,</span> self<span class="token punctuation">.</span>variance<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> loc_t<span class="token punctuation">,</span> conf_t<span class="token punctuation">,</span> idx<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 注意! 要清楚 Python 中的参数传递机制, 此处在函数内部会改变 loc_t, conf_t 的值, 关于 match 的详细讲解可以看后面的代码解析</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_gpu<span class="token punctuation">:</span>            loc_t <span class="token operator">=</span> loc_t<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>            conf_t <span class="token operator">=</span> conf_t<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 用Variable封装loc_t, 新版本的 PyTorch 无需这么做, 只需要将 requires_grad 属性设置为 True 就行了</span>        loc_t <span class="token operator">=</span> Variable<span class="token punctuation">(</span>loc_t<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        conf_t <span class="token operator">=</span> Variable<span class="token punctuation">(</span>conf_t<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        pos <span class="token operator">=</span> conf_t <span class="token operator">></span> <span class="token number">0</span> <span class="token comment" spellcheck="true"># 筛选出 >0 的box下标(大部分都是=0的)</span>        num_pos <span class="token operator">=</span> pos<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 求和, 取得满足条件的box的数量, [batch_size, num_gt_threshold]</span>        <span class="token comment" spellcheck="true"># 位置(localization)损失函数, 使用 Smooth L1 函数求损失</span>        <span class="token comment" spellcheck="true"># loc_data:[batch, num_priors, 4]</span>        <span class="token comment" spellcheck="true"># pos: [batch, num_priors]</span>        <span class="token comment" spellcheck="true"># pos_idx: [batch, num_priors, 4], 复制下标成坐标格式, 以便获取坐标值</span>        pos_idx <span class="token operator">=</span> pos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>pos<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>loc_data<span class="token punctuation">)</span>        loc_p <span class="token operator">=</span> loc_data<span class="token punctuation">[</span>pos_idx<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 获取预测结果值</span>        loc_t <span class="token operator">=</span> loc_t<span class="token punctuation">[</span>pos_idx<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 获取gt值</span>        loss_l <span class="token operator">=</span> F<span class="token punctuation">.</span>smooth_l1_loss<span class="token punctuation">(</span>loc_p<span class="token punctuation">,</span> loc_t<span class="token punctuation">,</span> size_average<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 计算损失</span>        <span class="token comment" spellcheck="true"># 计算最大的置信度, 以进行难负样本挖掘</span>        <span class="token comment" spellcheck="true"># conf_data: [batch, num_priors, num_classes]</span>        <span class="token comment" spellcheck="true"># batch_conf: [batch, num_priors, num_classes]</span>        batch_conf <span class="token operator">=</span> conf_data<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_classes<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># reshape</span>        <span class="token comment" spellcheck="true"># conf_t: [batch, num_priors]</span>        <span class="token comment" spellcheck="true"># loss_c: [batch*num_priors, 1], 计算每个priorbox预测后的损失</span>        loss_c <span class="token operator">=</span> log_sum_exp<span class="token punctuation">(</span>batch_conf<span class="token punctuation">)</span> <span class="token operator">-</span> batch_conf<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> conf_t<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 难负样本挖掘, 按照loss进行排序, 取loss最大的负样本参与更新</span>        loss_c<span class="token punctuation">[</span>pos<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment" spellcheck="true"># 将所有的pos下标的box的loss置为0(pos指示的是正样本的下标)</span>        <span class="token comment" spellcheck="true"># 将 loss_c 的shape 从 [batch*num_priors, 1] 转换成 [batch, num_priors]</span>        loss_c <span class="token operator">=</span> loss_c<span class="token punctuation">.</span>view<span class="token punctuation">(</span>num<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># reshape</span>        <span class="token comment" spellcheck="true"># 进行降序排序, 并获取到排序的下标</span>        _<span class="token punctuation">,</span> loss_idx <span class="token operator">=</span> loss_c<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> descending<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 将下标进行升序排序, 并获取到下标的下标</span>        _<span class="token punctuation">,</span> idx_rank <span class="token operator">=</span> loss_idx<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># num_pos: [batch, 1], 统计每个样本中的obj个数</span>        num_pos <span class="token operator">=</span> pos<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 根据obj的个数, 确定负样本的个数(正样本的3倍)</span>        num_neg <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>negpos_ratio<span class="token operator">*</span>num_pos<span class="token punctuation">,</span> max<span class="token operator">=</span>pos<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 获取到负样本的下标</span>        neg <span class="token operator">=</span> idx_rank <span class="token operator">&lt;</span> num_neg<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>idx_rank<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 计算包括正样本和负样本的置信度损失</span>        <span class="token comment" spellcheck="true"># pos: [batch, num_priors]</span>        <span class="token comment" spellcheck="true"># pos_idx: [batch, num_priors, num_classes]</span>        pos_idx <span class="token operator">=</span> pos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>conf_data<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># neg: [batch, num_priors]</span>        <span class="token comment" spellcheck="true"># neg_idx: [batch, num_priors, num_classes]</span>        neg_idx <span class="token operator">=</span> neg<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>conf_data<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 按照pos_idx和neg_idx指示的下标筛选参与计算损失的预测数据</span>        conf_p <span class="token operator">=</span> conf_data<span class="token punctuation">[</span><span class="token punctuation">(</span>pos_idx<span class="token operator">+</span>neg_idx<span class="token punctuation">)</span><span class="token punctuation">.</span>gt<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_classes<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 按照pos_idx和neg_idx筛选目标数据</span>        targets_weighted <span class="token operator">=</span> conf_t<span class="token punctuation">[</span><span class="token punctuation">(</span>pos<span class="token operator">+</span>neg<span class="token punctuation">)</span><span class="token punctuation">.</span>gt<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># 计算二者的交叉熵</span>        loss_c <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>conf_p<span class="token punctuation">,</span> targets_weighted<span class="token punctuation">,</span> size_average<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 将损失函数归一化后返回</span>        N <span class="token operator">=</span> num_pos<span class="token punctuation">.</span>data<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss_l <span class="token operator">=</span> loss_l <span class="token operator">/</span> N        loss_c <span class="token operator">=</span> loss_c <span class="token operator">/</span> N        <span class="token keyword">return</span> loss_l<span class="token punctuation">,</span> loss_c</code></pre><h4 id="确定default-boxes的尺度以及比率"><a href="#确定default-boxes的尺度以及比率" class="headerlink" title="确定default boxes的尺度以及比率"></a>确定default boxes的尺度以及比率</h4><p>实验表明使用低层的feature maps可以提高语义分割质量,因为底层的feature maps包含输入目标的细节信息.<br>来自于一个网络不同层的feature maps具有不同的感受野,在SSD网络中,default boxes不需要与每一层的真实的感受野严格匹配.作者将default boxes的大小设计为与特定尺度的目标相关联.假设在模型中使用了m层特征图.每一层特征图的default boxes的尺度计算如下:</p><center><br>$s _ { k } = s _ { \min } + \frac { s _ { \max } - s _ { \min } } { m - 1 } ( k - 1 ) , \quad k \in [ 1 , m ]$<br></center><p>其中$s_{min}$为$0.2$,$s_{max}$为$0.9$,分别代表最低、最高层的尺度.<br>对于,每一层中的default又引入了五种不同的比率,即:</p><center><br>${ 1,2,3 , \frac { 1 } { 2 } , \frac { 1 } { 3 }}$<br></center><p>依据比率，得出宽的计算公式为:</p><center><br>$w _ { k } ^ { a } = s _ { k } \sqrt { a _ { r } }$<br></center><p>高的计算公式为:</p><center><br>$h _ { k } ^ { a } = s _ { k } / \sqrt { a _ { r } }$<br></center><p>对于比率1,又额外定义了一个尺度,计算如下:</p><center><br>$s _ { k } ^ { \prime } = \sqrt { S _ { k } S _ { k + 1 } }$<br></center><p>这样,每一层特征图的每一个位置上便有六个不同比率的default boxes,将每一个位置上的6个default boxes的中心坐标设置为:</p><center><br>$\left( \frac { i + 0.5 } { \left| f _ { k } \right| } , \frac { j + 0.5 } { \left| f _ { k } \right| } \right)$<br></center><p>其中 $\left| f _ { k } \right|$表示第k个特征图的大小，$i , j \in \left[ 0 , \left| f _ { k } \right|\right.]$,对应特征图上所有可能的位置点.</p><p>实际中,不同的数据集适用于不同的尺度以及比例,若数据集中包含有更多的小目标,则需要设计更多的小尺度default boxes,相应的,若包含有更多的大目标,则需要设计更多的大尺度default boxes.<br>实现这一功能的代码如下:借鉴自:<a href="https://hellozhaozheng.github.io/z_post/PyTorch-SSD/#MultiBox," title="代码出处" target="_blank" rel="noopener">代码出处 </a></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># `layers/functions/prior_box.py`</span><span class="token keyword">class</span> <span class="token class-name">PriorBox</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 所谓priorbox实际上就是网格中每一个cell推荐的box</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> cfg<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 在SSD的init中, cfg=(coco, voc)[num_classes=21]</span>        <span class="token comment" spellcheck="true"># coco, voc的相关配置都来自于data/cfg.py 文件</span>        super<span class="token punctuation">(</span>PriorBox<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>image_size <span class="token operator">=</span> cfg<span class="token punctuation">[</span><span class="token string">"min_dim"</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>num_priors <span class="token operator">=</span> len<span class="token punctuation">(</span>cfg<span class="token punctuation">[</span><span class="token string">"aspect_ratios"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>variance <span class="token operator">=</span> cfg<span class="token punctuation">[</span><span class="token string">"variance"</span><span class="token punctuation">]</span> <span class="token operator">or</span> <span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>min_sizes <span class="token operator">=</span> cfg<span class="token punctuation">[</span><span class="token string">"min_sizes"</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>max_sizes <span class="token operator">=</span> cfg<span class="token punctuation">[</span><span class="token string">"max_sizes"</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>steps <span class="token operator">=</span> cfg<span class="token punctuation">[</span><span class="token string">"steps"</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>aspect_ratios <span class="token operator">=</span> cfg<span class="token punctuation">[</span><span class="token string">"aspect_ratios"</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>clip <span class="token operator">=</span> cfg<span class="token punctuation">[</span><span class="token string">"clip"</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>version <span class="token operator">=</span> cfg<span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> v <span class="token keyword">in</span> self<span class="token punctuation">.</span>variance<span class="token punctuation">:</span>            <span class="token keyword">if</span> v <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>                <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Variances must be greater than 0"</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        mean <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> k<span class="token punctuation">,</span> f <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>self<span class="token punctuation">.</span>feature_maps<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 存放的是feature map的尺寸:38,19,10,5,3,1</span>            <span class="token comment" spellcheck="true"># from itertools import product as product</span>            <span class="token keyword">for</span> i<span class="token punctuation">,</span> j <span class="token keyword">in</span> product<span class="token punctuation">(</span>range<span class="token punctuation">(</span>f<span class="token punctuation">)</span><span class="token punctuation">,</span> repeat<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># 这里实际上可以用最普通的for循环嵌套来代替, 主要目的是产生anchor的坐标(i,j)</span>                f_k <span class="token operator">=</span> self<span class="token punctuation">.</span>image_size <span class="token operator">/</span> self<span class="token punctuation">.</span>steps<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># steps=[8,16,32,64,100,300]. f_k大约为feature map的尺寸</span>                <span class="token comment" spellcheck="true"># 求得center的坐标, 浮点类型. 实际上, 这里也可以直接使用整数类型的 `f`, 计算上没太大差别</span>                cx <span class="token operator">=</span> <span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token operator">/</span> f_k                cy <span class="token operator">=</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token operator">/</span> f_k <span class="token comment" spellcheck="true"># 这里一定要特别注意 i,j 和cx, cy的对应关系, 因为cy对应的是行, 所以应该零cy与i对应.</span>                <span class="token comment" spellcheck="true"># aspect_ratios 为1时对应的box</span>                s_k <span class="token operator">=</span> self<span class="token punctuation">.</span>min_sizes<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token operator">/</span>self<span class="token punctuation">.</span>image_size                mean <span class="token operator">+=</span> <span class="token punctuation">[</span>cx<span class="token punctuation">,</span> cy<span class="token punctuation">,</span> s_k<span class="token punctuation">,</span> s_k<span class="token punctuation">]</span>                <span class="token comment" spellcheck="true"># 根据原文, 当 aspect_ratios 为1时, 会有一个额外的 box, 如下:</span>                <span class="token comment" spellcheck="true"># rel size: sqrt(s_k * s_(k+1))</span>                s_k_prime <span class="token operator">=</span> sqrt<span class="token punctuation">(</span>s_k <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>max_sizes<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token operator">/</span>self<span class="token punctuation">.</span>image_size<span class="token punctuation">)</span><span class="token punctuation">)</span>                mean <span class="token operator">+=</span> <span class="token punctuation">[</span>cx<span class="token punctuation">,</span> cy<span class="token punctuation">,</span> s_k_prime<span class="token punctuation">,</span> s_k_prime<span class="token punctuation">]</span>                <span class="token comment" spellcheck="true"># 其余(2, 或 2,3)的宽高比(aspect ratio)</span>                <span class="token keyword">for</span> ar <span class="token keyword">in</span> self<span class="token punctuation">.</span>aspect_ratios<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">:</span>                    mean <span class="token operator">+=</span> <span class="token punctuation">[</span>cx<span class="token punctuation">,</span> cy<span class="token punctuation">,</span> s_k<span class="token operator">*</span>sqrt<span class="token punctuation">(</span>ar<span class="token punctuation">)</span><span class="token punctuation">,</span> s_k<span class="token operator">/</span>sqrt<span class="token punctuation">(</span>ar<span class="token punctuation">)</span><span class="token punctuation">]</span>                    mean <span class="token operator">+=</span> <span class="token punctuation">[</span>cx<span class="token punctuation">,</span> cy<span class="token punctuation">,</span> s_k<span class="token operator">/</span>sqrt<span class="token punctuation">(</span>ar<span class="token punctuation">)</span><span class="token punctuation">,</span> s_k<span class="token operator">*</span>sqrt<span class="token punctuation">(</span>ar<span class="token punctuation">)</span><span class="token punctuation">]</span>                <span class="token comment" spellcheck="true"># 综上, 每个卷积特征图谱上每个像素点最终产生的 box 数量要么为4, 要么为6, 根据不同情况可自行修改.</span>        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>mean<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>clip<span class="token punctuation">:</span>            output<span class="token punctuation">.</span>clamp_<span class="token punctuation">(</span>max<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> min<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># clamp_ 是clamp的原地执行版本</span>        <span class="token keyword">return</span> output <span class="token comment" spellcheck="true"># 输出default box坐标(可以理解为anchor box)</span></code></pre><h4 id="Hard-negative-mining"><a href="#Hard-negative-mining" class="headerlink" title="Hard negative mining"></a>Hard negative mining</h4><p>这一策略主要用于解决正负样本数目不均衡的问题,在进行边框匹配之后,大多数的default boxes都是负样本.这一结果会导致样本不平衡.因而在训练时,没有使用所有的负样本,而是首先依据每一个default box的置信度损失进行排序,选出最高的几个,使得正负样本的比例为1:3.使用选出的样本进行训练.</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><table><thead><tr><th>模型</th><th>mAP</th><th>FPS</th></tr></thead><tbody><tr><td>SSD 300</td><td>74.3%</td><td>59</td></tr><tr><td>SSD 512</td><td>76.9%</td><td>-</td></tr><tr><td>Faster R-CNN</td><td>73.2%</td><td>7</td></tr><tr><td>YOLOV1</td><td>63.4%</td><td>45</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ResNet论文解读</title>
      <link href="/2019/04/30/resnet-lun-wen-jie-du/"/>
      <url>/2019/04/30/resnet-lun-wen-jie-du/</url>
      
        <content type="html"><![CDATA[<h2 id="Deep-Residual-Learning-for-Image-Recogition"><a href="#Deep-Residual-Learning-for-Image-Recogition" class="headerlink" title="Deep Residual Learning for Image Recogition"></a>Deep Residual Learning for Image Recogition</h2><hr><h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2><p><strong>Kaiming He</strong>    Xiangyu Zhang       Shaoqing Ren    Jian Sun</p><p><strong>何凯明大佬</strong></p><center><br><br><img src="http://kaiminghe.com/img/me.jpg" alt=""><br><br></center><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>这篇论文主要用于解决网络层数加深时,模型的训练问题.</p><h3 id="深度网络的退化问题"><a href="#深度网络的退化问题" class="headerlink" title="深度网络的退化问题"></a>深度网络的退化问题</h3><p>如下图所示,为历年ISLVRC竞赛中取得冠军的各个网络结构,观察该图片可知,随着网络层数的增加,模型的复杂度不断提升,进而可以提取更为丰富的特征,因而也得到了更好的结果.</p><center><br><img src="/2019/04/30/resnet-lun-wen-jie-du/ISLVRC冠军结构.jpg" title="ISLVRC冠军结构"><br></center><p>但事实上,模型的表达能力和模型复杂度并不是成正比关系的,在论文中,作者指出,随着深度的增加,模型出现了退化问题(Degradation problem),如下图所示.</p><center><br><br><img src="/2019/04/30/resnet-lun-wen-jie-du/模型退化问题.jpg" title="模型退化问题"><br><br></center><p>网络深度增加时，网络准确度出现饱和，甚至出现下降.这一问题并不是由过拟合所导致的,因为在图中,56层网络的训练误差同样很大.</p><p>在深层网络存在着梯度消失或者爆炸的问题，这使得深度学习模型很难训练。但是现在已经存在一些技术手段,如BatchNorm来缓解这个问题。因此，出现深度网络的退化问题是非常令人诧异的。</p><h3 id="残差学习"><a href="#残差学习" class="headerlink" title="残差学习"></a>残差学习</h3><p>深度网络的退化问题至少说明深度网络不容易训练。但是我们考虑这样一个事实：现在你有一个浅层网络，你想通过向上堆积新层来建立深层网络，一个极端情况是这些增加的层什么也不学习，仅仅复制浅层网络的特征，即这样新层是恒等映射（Identity mapping）。在这种情况下，深层网络应该至少和浅层网络性能一样，不应该出现退化现象。</p><p>为了解决这一问题,在本文中作者提出了残差学习的思想.对于一个堆积层结构（几层堆积而成）当输入为$x$时其学习到的特征记为$H(x)$，现在我们希望其可以学习到残差$F(x)=H(x)-x$，这样其实原始的学习特征是$F(x)+x$。之所以这样是因为残差学习相比原始特征直接学习更容易。当残差为0时，此时堆积层仅仅做了恒等映射，至少网络性能不会下降，实际上残差不会为0，这也会使得堆积层在输入特征基础上学习到新的特征，从而拥有更好的性能。残差学习的结构如下图所示。这有点类似与电路中的“短路”，所以是一种短路连接（shortcut connection）。</p><center><br><br><img src="/2019/04/30/resnet-lun-wen-jie-du/残差模块.jpg" title="残差模块"><br><br></center><p>从数学角度解释残差学习更为容易的原因.</p><p>将残差单元表示为:</p><center><br><br>$y_l=h(x_l)+F(x_l, W_l)$<br><br>$x_{l+1}=f(y_l)$<br><br></center><p>其中,$h(x_l)$表示恒等映射,$F(x_l, W_l)$表示残差映射,$f$为ReLU激活函数,那么从浅层$l$到深层$L$的特征为:</p><center><br><br>$x_L=x_l+\sum_{i=l}^{L-1}F(x_i, W_i)$<br><br></center><p>使用链式求导法则可以得到损失相对于第$x_l$层的梯度为:</p><center><br><br>$\frac{\partial loss}{\partial x_l} = \frac{\partial loss}{partial x_L} \cdot \frac{\partial x_L}{\partial x_l} =  \frac{\partial loss}{\partial x_L} \cdot (1 + \frac{\partial}{\partial x_l} \sum_{i=l}^{L-1}F(x_i, W_i))$<br><br></center><p>由上式我们可以发现,小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易(上面的推导并不是严格的证明)。</p><h3 id="ResNet的网络结构"><a href="#ResNet的网络结构" class="headerlink" title="ResNet的网络结构"></a>ResNet的网络结构</h3><p>ResNet网络是参考了VGG19网络，在其基础上进行了修改，并通过短路机制加入了残差单元，如图所示。变化主要体现在ResNet直接使用stride=2的卷积做下采样，并且用global average pool层替换了全连接层。ResNet的一个重要设计原则是：<strong>当feature map大小降低一半时，feature map的数量增加一倍</strong>，这保持了网络层的复杂度。从图中可以看到，ResNet相比普通网络每两层间增加了短路机制，这就形成了残差学习，其中虚线表示feature map数量发生了改变。图中展示的34-layer的ResNet，还可以构建更深的网络如表所示。从表中可以看到，对于18-layer和34-layer的ResNet，其进行的两层间的残差学习，当网络更深时，其进行的是三层间的残差学习，三层卷积核分别是1x1，3x3和1x1，一个值得注意的是隐含层的feature map数量是比较小的，并且是输出feature map数量的1/4。</p><center><br><img src="/2019/04/30/resnet-lun-wen-jie-du/resnet结构.jpg" title="resnet结构"><br>ResNet网络结构<br><br></center><center><br><img src="/2019/04/30/resnet-lun-wen-jie-du/不同的resnet结构.jpg" title="不同的resnet结构"><br>不同的ResNet网络结构<br><br></center><h4 id="残差单元"><a href="#残差单元" class="headerlink" title="残差单元"></a>残差单元</h4><p>ResNet使用两种残差单元，如图所示。左图对应的是浅层网络，而右图对应的是深层网络。对于短路连接，当输入和输出维度一致时，可以直接将输入加到输出上。但是当维度不一致时（对应的是维度增加一倍），这就不能直接相加。有两种策略：（1）采用zero-padding增加维度，此时一般要先做一个downsample，可以采用strde=2的pooling，这样不会增加参数；（2）采用新的映射（projection shortcut），一般采用1x1的卷积(可以改变维度)，这样会增加参数，也会增加计算量。短路连接除了直接使用恒等映射，当然都可以采用projection shortcut。</p><center><br><img src="/2019/04/30/resnet-lun-wen-jie-du/残差单元.jpg" title="残差单元"><br>残差单元示意图(左侧为浅层网络使用的残差单元,右侧为深层网络)<br><br></center><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>如图所示,左侧为不使用残差模块的普通深度网络,右侧为ResNet.</p><center><br><img src="/2019/04/30/resnet-lun-wen-jie-du/结果对比.jpg" title="结果对比"><br>结果示意图<br><br></center><p>从图中我们可以看出,在左侧,未使用残差模块的网络明显出现了退化现象,而右侧则无此问题.</p><h2 id="一种更为优秀的残差模块"><a href="#一种更为优秀的残差模块" class="headerlink" title="一种更为优秀的残差模块"></a>一种更为优秀的残差模块</h2><p>采用前置激活可以提升残差模块的性能,如图所示:</p><center><br><img src="/2019/04/30/resnet-lun-wen-jie-du/残差模块改进.jpg" title="残差模块改进"><br><br></center>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> 经典网络结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/01/27/hello-world/"/>
      <url>/2019/01/27/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
